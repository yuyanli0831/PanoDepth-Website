<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet"
        integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Poppins:wght@100;400;700&display=swap" rel="stylesheet">
    <title>PanoDepth</title>
    <style>
        * {
            font-family: 'Poppins', sans-serif;
        }

        .abstract {
            text-align: justify;
        }

        a {
            text-decoration: none;
        }

        h1 {
            font-weight: bold;
        }
        .wrapper {
            text-align: center;

        }
    </style>
</head>

<body>

    <nav class="sticky-top navbar navbar-expand-lg navbar-light bg-light">
        <div class="container">
            <a class="navbar-brand" href="/">PanoDepth</a>
            <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNav"
                aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
                <span class="navbar-toggler-icon"></span>
            </button>
            <div class="collapse navbar-collapse" id="navbarNav">
                <ul class="navbar-nav">
                    <li class="nav-item">
                        <strong><a class="nav-link"
                                href="https://ieeexplore.ieee.org/document/9665896">Paper</a></strong>
                    </li>
                    <li class="nav-item">
                        <strong><a class="nav-link" href="https://arxiv.org/abs/2202.01323">arXiv</a></strong>
                    </li>
                    <li class="nav-item">
                        <strong><a class="nav-link" href="#">GitHub</a></strong>
                    </li>
                    <li class="nav-item">
                        <strong><a class="nav-link" href="https://youtu.be/GGgpT4odBs4">YouTube</a></strong>
                    </li>
                </ul>
            </div>

        </div>
    </nav>


    <div class="container">
        <div class="row">
            <div class="col-md-12 mt-5">
                <h1 class="text-center">PanoDepth</h1>
                <h2 class="text-center">A Two Stage Approach for Monocular Omnidirectional Depth Estimation</h2>
                <!-- <h4 class="text-center">
                    <a href="#">Yuyan Li</a><sup>1</sup>,
                    <a href="#">Zhixin Yan</a><sup>2</sup>,
                    <a href="#">Ye Duan</a><sup>1</sup>
                    <a href="#">Liu Ren</a><sup>2</sup>,
                </h4>
                <p class="text-center">
                    <sup>1</sup>University of Missouri, <sup>2</sup>Bosch Research
                </p>
                <p class="text-center">
                    <img width="200" class="p-2" src="images/mizzou.png" alt="">
                    <img width="200" class="p-2" src="images/bnosch.png" alt="">
                </p> -->
                <div class="wrapper">
                    <div class="btn-group btn-group-lg mr-2 text-center" role="group" aria-label="First group">
                        <a href="https://ieeexplore.ieee.org/document/9665896" class="btn btn-outline-primary">Paper</a>
                        <a href="https://arxiv.org/abs/2202.01323" class="btn btn-outline-primary">arXiv</a>
                        <a href="#" class="btn btn-outline-primary">GitHub</a>
                        <a href="https://youtu.be/GGgpT4odBs4" class="btn btn-outline-primary">YouTube</a>
                    </div>
                </div>
                <h2>Abstract</h2>
                <p class="abstract">
                    Omnidirectional 3D information is essential for a wide range of applications such as Virtual
                    Reality,
                    Autonomous Driving, Robotics, etc. In this paper, we propose a novel, model-agnostic, two-stage
                    pipeline
                    for omnidirectional monocular depth estimation. Our proposed framework PanoDepth takes one 360 image
                    as
                    input, produces one or more synthesized views in the first stage, and feeds the original image and
                    the
                    synthesized images into the subsequent stereo matching stage. In the second stage, we propose a
                    differentiable Spherical Warping Layer to handle omnidirectional stereo geometry efficiently and
                    effectively. By utilizing the explicit stereo-based geometric constraints in the stereo matching
                    stage,
                    PanoDepth can generate dense high-quality depth. We conducted extensive experiments and ablation
                    studies
                    to evaluate PanoDepth with both the full pipeline as well as the individual modules in each stage.
                    Our
                    results show that PanoDepth outperforms the state-of-the-art approaches by a large margin for 360
                    monocular depth estimation.</p>

                <!-- <p class="text-center">
                    <img style="max-width: 800px; width: 100%;" src="images/poster.png" alt="">
                </p> -->

                <figure style="max-width: 800px; width: 100%; display: block; margin: 0 auto;" class="figure mb-5 mt-5">
                    <img src="images/poster.png" class="figure-img img-fluid rounded"
                        alt="General diagram of PanoDepth.">
                    <figcaption class="figure-caption">
                        <strong>Fig. </strong>Illustration of our PanoDepth framework. PanoDepth takes one 360 image as
                        input to generate one or more novel views in
                        the first view synthesis stage. The original and synthesized 360 images are then fed into the
                        second multi-view stereo matching stage to
                        predict final dense depth map.
                    </figcaption>
                </figure>
                <figure style="max-width: 800px; width: 100%; display: block; margin: 0 auto;" class="figure mb-5 mt-5">
                    <img src="images/comparison.png" class="figure-img img-fluid rounded"
                        alt="Result comparison with other methods.">
                    <figcaption class="figure-caption">
                        <strong>Fig. </strong>A qualitative comparison between our method (3rd column) BiFuse (4th
                        column), and OmniDepth (5th column) on 360D. We highlight and zoom in some areas that
                        distinguish the performance of three methods. We can see that our PanoDepth is able to produce
                        sharp edges, predict depth range accurately, and recover surface detail.
                    </figcaption>
                </figure>


            </div>
        </div>

        <div class="row">

            <div class="col-md-4">
                <img src="images/Picture1.png" class="rounded mx-auto d-block" alt="PanoDepth Result Example in 3D.">

                <figure class="figure">
                    <img src="images/Media1.gif" class="figure-img img-fluid rounded"
                        alt="PanoDepth Result Example in 3D.">
                </figure>
            </div>
            <div class="col-md-4">
                <img src="images/Picture2.png" class="rounded mx-auto d-block" alt="PanoDepth Result Example in 3D.">
                <figure class="figure">
                    <img src="images/Media2.gif" class="figure-img img-fluid rounded"
                        alt="PanoDepth Result Example in 3D.">
                </figure>
            </div>
            <div class="col-md-4">
                <img src="images/Picture3.png" class="rounded mx-auto d-block" alt="PanoDepth Result Example in 3D.">
                <figure class="figure">
                    <img src="images/Media3.gif" class="figure-img img-fluid rounded"
                        alt="PanoDepth Result Example in 3D.">
                </figure>
            </div>
            <div class="col-md-12">
                <figcaption class="figure-caption text-center">
                    <strong>Fig. </strong>3D point cloud reconstructed from estimated depth using PanoDepth.
                </figcaption>
            </div>

            <div class="col-md-12 p-5">
                <div class="text-center">
                    <img style="max-width: 400px; width: 100%;" class="figure-img img-fluid rounded" src="images/fullposter.png" alt="">
                </div>
                <div class="text-center">
                    <a href="https://github.com/yuyanli0831/PanoDepth-Website/raw/main/downloads/panodepth_poster_v2.pdf">Download poster [pdf]</a>
                </div>
            </div>

        </div>

        <div class="row">
            <h2>Cite</h2>
            <div class="bg-light p-4">
                <pre>@inproceedings{li2021panodepth,
    author={Li, Yuyan and Yan, Zhixin and Duan, Ye and Ren, Liu},
    booktitle={2021 International Conference on 3D Vision (3DV)}, 
    title={PanoDepth: A Two-Stage Approach for Monocular Omnidirectional Depth Estimation}, 
    year={2021},
    volume={},
    number={},
    pages={648-658},
    doi={10.1109/3DV53792.2021.00074}
}</pre>
            </div>

        </div>
    </div>

    <div class="container">
        <!-- <p class="text-center p-5">
            Copyright Â© Yuyan Li 2022
        </p> -->
    </div>
    <script src="https://code.jquery.com/jquery-3.6.0.min.js"
        integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.min.js"
        integrity="sha384-QJHtvGhmr9XOIpI6YVutG+2QOK9T+ZnN4kzFN1RtK3zEFEIsxhlmWl5/YESvpZ13"
        crossorigin="anonymous"></script>

</body>

</html>